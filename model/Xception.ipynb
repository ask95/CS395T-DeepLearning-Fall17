{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Dropout, Flatten, Activation, Lambda, Input\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Lambda\n",
    "from keras import initializers\n",
    "from keras.utils import get_file\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "data_path = '/home/05148/picsou/project1/data/yearbook/'\n",
    "\n",
    "f = open(data_path + 'yearbook_train.txt', 'r')\n",
    "\n",
    "freq = {};\n",
    "normal_const = 0;\n",
    "\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    image, year = line.split(\"\\t\")\n",
    "    if year in freq:\n",
    "        freq[year] += 1\n",
    "    else:\n",
    "        freq[year] = 1\n",
    "\n",
    "normal_const = np.sum(freq.values())\n",
    "for key in freq:\n",
    "    freq[key] = freq[key]/float(normal_const);\n",
    "    \n",
    "sorted_freq = collections.OrderedDict(sorted(freq.items()))\n",
    "\n",
    "idx = 0;\n",
    "class_weights_train = {}\n",
    "idx2year = {}\n",
    "\n",
    "for key in sorted_freq:\n",
    "    class_weights_train[idx] = sorted_freq[key]\n",
    "    idx2year[idx] = key\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.image import apply_transform, transform_matrix_offset_center\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import random_rotation, random_shear, random_shift, random_zoom\n",
    "from skimage import exposure\n",
    "\n",
    "class RegressDataGen:\n",
    "    def __init__(self, directory, map_file, target_size = (171, 186, 3), \n",
    "                 class_weights_train = None, multi_output=False, do_augmentation=True, \n",
    "                 samplewise_center = True,\n",
    "                 samplewise_std_deviation = True,\n",
    "                 multi_input=False\n",
    "                ):\n",
    "        self.directory = directory\n",
    "        self.map_file = map_file\n",
    "        self.filenames = []\n",
    "        self.map = {}\n",
    "        self.fnameToGender = {}\n",
    "        self.target_size = target_size\n",
    "        self.populate_filenames()\n",
    "        self.populate_mapping()\n",
    "        self.regressIter = None\n",
    "        self.steps = 0\n",
    "        self.samplewise_center = samplewise_center\n",
    "        self.samplewise_std_deviation = samplewise_std_deviation\n",
    "        self.height_shift_range = 0.2\n",
    "        self.width_shift_range = 0.2\n",
    "        self.max_rotation = 45\n",
    "        self.shear = 0.785398\n",
    "        self.zoom_range = (0.5, 0.5)\n",
    "        self.do_augmentation = do_augmentation\n",
    "        self.class_weights_train = class_weights_train\n",
    "        self.equalizehist = False\n",
    "        self.multi_output = multi_output\n",
    "        self.multi_input = multi_input\n",
    "        self.lastN = []\n",
    "        \n",
    "    def _recursive_list(self, subpath):\n",
    "        return sorted(\n",
    "            os.walk(subpath, followlinks=False), key=lambda tpl: tpl[0])\n",
    "    \n",
    "    def populate_mapping(self):\n",
    "        f = open(self.map_file, 'r')\n",
    "\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            image, year = line.split(\"\\t\")\n",
    "            gender, imfilename = image.split(\"/\")\n",
    "            if gender is 'M':\n",
    "                encodeGender = 1\n",
    "            elif gender is 'F':\n",
    "                encodeGender = 0\n",
    "            self.fnameToGender[image] = encodeGender\n",
    "            self.map[image] = year\n",
    "            \n",
    "    def populate_filenames(self):\n",
    "        base_dir = self.directory\n",
    "        for root, _, files in self._recursive_list(base_dir):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith('.' + 'png'):\n",
    "                    self.filenames.append(os.path.relpath(os.path.join(root, fname), base_dir))\n",
    "                    \n",
    "    def preprocess(self, x):\n",
    "        if self.equalizehist:\n",
    "            print(x)\n",
    "            x = exposure.equalize_hist(x)\n",
    "            print(x)\n",
    "            exit()\n",
    "        return x\n",
    "            \n",
    "    def augment_data(self, x):\n",
    "        \n",
    "        x = random_shift(x, self.width_shift_range, self.height_shift_range, \n",
    "                         row_axis=0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_rotation(x, self.max_rotation, \n",
    "                            row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_shear(x, self.shear, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_zoom(x, self.zoom_range, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "    def flow_from_directory(self, batch_size = 32, shuffle = True, seed = 42):\n",
    "        \n",
    "        self.regressIter = Iterator(len(self.filenames), batch_size = batch_size, shuffle = shuffle, seed = seed)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            factor = 3\n",
    "        else:\n",
    "            factor = 1\n",
    "        \n",
    "        self.steps = math.ceil(len(self.filenames)/batch_size) * factor\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def next(self, *args, **kwargs):\n",
    "           \n",
    "        self.lastN = []\n",
    "        \n",
    "        idx_array, cur_idx, bs = next(self.regressIter.index_generator)\n",
    "        \n",
    "        batch_x = np.zeros(tuple([len(idx_array)] + list(self.target_size)), dtype=K.floatx())\n",
    "        \n",
    "        batch_y = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        if self.multi_output:\n",
    "            batch_y_gender = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "    \n",
    "        if self.multi_input:\n",
    "            batch_x_gender = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        if self.class_weights_train is not None:\n",
    "            sample_weights = np.ones(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        for i, j in enumerate(idx_array):\n",
    "            fname = self.filenames[j]\n",
    "            self.lastN.append(fname)\n",
    "            img = load_img(\n",
    "                  os.path.join(self.directory, fname),\n",
    "                  grayscale = True,\n",
    "                  target_size= self.target_size)\n",
    "            x = np.array(img_to_array(img, data_format='channels_last'))\n",
    "            x = self.preprocess(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = self.map[fname]\n",
    "            \n",
    "            if self.multi_output:\n",
    "                batch_y_gender[i] = self.fnameToGender[fname]\n",
    "            \n",
    "            if self.multi_input:\n",
    "                batch_x_gender[i] = self.fnameToGender[fname]\n",
    "            \n",
    "            if self.class_weights_train is not None:\n",
    "                if self.multi_output:\n",
    "                    sample_weights[i] = self.class_weights_train[batch_y[i].astype('int').astype('str')]\n",
    "                else:\n",
    "                    sample_weights[i] = self.class_weights_train[batch_y[i].astype('int').astype('str')]\n",
    "        \n",
    "        if self.samplewise_center:\n",
    "            for x in batch_x:\n",
    "                x -= np.mean(x)\n",
    "        \n",
    "        if self.samplewise_std_deviation:\n",
    "            for x in batch_x:\n",
    "                x /= np.std(x)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            for x in batch_x:\n",
    "                x = self.augment_data(x)\n",
    "        \n",
    "        if self.multi_output:\n",
    "            if self.class_weights_train is not None:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}, {'out_year' : sample_weights, 'out_gender' : sample_weights} \n",
    "            else:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}\n",
    "            \n",
    "        elif self.multi_input:\n",
    "            if self.class_weights_train is not None:\n",
    "                return {'input_1' : batch_x, 'input_2': batch_x_gender}, batch_y, sample_weights\n",
    "            else:\n",
    "                return {'input_1' : batch_x, 'input_2': batch_x_gender}, batch_y\n",
    "        else:    \n",
    "            if self.class_weights_train is not None:\n",
    "                return (batch_x, batch_y, sample_weights)\n",
    "            else:\n",
    "                return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "train = RegressDataGen(data_path + 'train',\n",
    "                       data_path + 'yearbook_train.txt', \n",
    "                       class_weights_train = sorted_freq,\n",
    "                       do_augmentation = False,\n",
    "                      )\n",
    "valid = RegressDataGen(data_path + 'valid',\n",
    "                       data_path + 'yearbook_valid.txt',\n",
    "                       class_weights_train = sorted_freq, \n",
    "                       do_augmentation = False,\n",
    "                      )\n",
    "\n",
    "train = train.flow_from_directory()\n",
    "valid = valid.flow_from_directory(shuffle=False)\n",
    "\n",
    "mean_value = 0\n",
    "for key in freq:\n",
    "    mean_value += freq[key] * float(key)\n",
    "    \n",
    "pretrained_model = Xception(include_top=False, weights='imagenet', input_shape=(171, 186, 3))\n",
    "x = pretrained_model.output\n",
    "x = Conv2D(8, (1, 1), activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation='relu', bias_initializer=keras.initializers.Ones())(x)\n",
    "#x = BatchNormalization()(x)\n",
    "predicted_year = Dense(1, bias_initializer = keras.initializers.Constant(mean_value))(x)\n",
    "\n",
    "model = Model(inputs=pretrained_model.input, outputs=predicted_year)\n",
    "\n",
    "lr = 1e-3\n",
    "def lr_schedule(epoch):\n",
    "    return lr * (0.1 ** float(epoch / 10.0))\n",
    "\n",
    "model.compile(Adam(lr=lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[[-0.7447266 , -0.7447266 , -0.7447266 ],\n",
      "         [-0.72909158, -0.72909158, -0.72909158],\n",
      "         [-0.72909158, -0.72909158, -0.72909158],\n",
      "         ..., \n",
      "         [-0.38512161, -0.38512161, -0.38512161],\n",
      "         [-0.3694866 , -0.3694866 , -0.3694866 ],\n",
      "         [-0.35385162, -0.35385162, -0.35385162]],\n",
      "\n",
      "        [[-0.7447266 , -0.7447266 , -0.7447266 ],\n",
      "         [-0.7447266 , -0.7447266 , -0.7447266 ],\n",
      "         [-0.71345657, -0.71345657, -0.71345657],\n",
      "         ..., \n",
      "         [-0.38512161, -0.38512161, -0.38512161],\n",
      "         [-0.3694866 , -0.3694866 , -0.3694866 ],\n",
      "         [-0.3694866 , -0.3694866 , -0.3694866 ]],\n",
      "\n",
      "        [[-0.7447266 , -0.7447266 , -0.7447266 ],\n",
      "         [-0.72909158, -0.72909158, -0.72909158],\n",
      "         [-0.71345657, -0.71345657, -0.71345657],\n",
      "         ..., \n",
      "         [-0.3694866 , -0.3694866 , -0.3694866 ],\n",
      "         [-0.3694866 , -0.3694866 , -0.3694866 ],\n",
      "         [-0.3694866 , -0.3694866 , -0.3694866 ]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.06829338,  0.06829338,  0.06829338],\n",
      "         [ 0.06829338,  0.06829338,  0.06829338],\n",
      "         [ 0.08392838,  0.08392838,  0.08392838],\n",
      "         ..., \n",
      "         [-0.29131162, -0.29131162, -0.29131162],\n",
      "         [-0.22877161, -0.22877161, -0.22877161],\n",
      "         [-0.22877161, -0.22877161, -0.22877161]],\n",
      "\n",
      "        [[ 0.06829338,  0.06829338,  0.06829338],\n",
      "         [ 0.06829338,  0.06829338,  0.06829338],\n",
      "         [ 0.06829338,  0.06829338,  0.06829338],\n",
      "         ..., \n",
      "         [-0.30694661, -0.30694661, -0.30694661],\n",
      "         [-0.22877161, -0.22877161, -0.22877161],\n",
      "         [-0.19750161, -0.19750161, -0.19750161]],\n",
      "\n",
      "        [[ 0.06829338,  0.06829338,  0.06829338],\n",
      "         [ 0.06829338,  0.06829338,  0.06829338],\n",
      "         [ 0.06829338,  0.06829338,  0.06829338],\n",
      "         ..., \n",
      "         [-0.32258162, -0.32258162, -0.32258162],\n",
      "         [-0.24440661, -0.24440661, -0.24440661],\n",
      "         [-0.19750161, -0.19750161, -0.19750161]]],\n",
      "\n",
      "\n",
      "       [[[ 0.38849702,  0.38849702,  0.38849702],\n",
      "         [ 0.38849702,  0.38849702,  0.38849702],\n",
      "         [ 0.37116012,  0.37116012,  0.37116012],\n",
      "         ..., \n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ],\n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ],\n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ]],\n",
      "\n",
      "        [[ 0.40583393,  0.40583393,  0.40583393],\n",
      "         [ 0.38849702,  0.38849702,  0.38849702],\n",
      "         [ 0.37116012,  0.37116012,  0.37116012],\n",
      "         ..., \n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ],\n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ],\n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ]],\n",
      "\n",
      "        [[ 0.40583393,  0.40583393,  0.40583393],\n",
      "         [ 0.38849702,  0.38849702,  0.38849702],\n",
      "         [ 0.37116012,  0.37116012,  0.37116012],\n",
      "         ..., \n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ],\n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ],\n",
      "         [ 1.3073535 ,  1.3073535 ,  1.3073535 ]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.50985539,  0.50985539,  0.50985539],\n",
      "         [ 0.50985539,  0.50985539,  0.50985539],\n",
      "         [ 0.50985539,  0.50985539,  0.50985539],\n",
      "         ..., \n",
      "         [ 0.12844332,  0.12844332,  0.12844332],\n",
      "         [ 0.16311714,  0.16311714,  0.16311714],\n",
      "         [ 0.19779097,  0.19779097,  0.19779097]],\n",
      "\n",
      "        [[ 0.52719235,  0.52719235,  0.52719235],\n",
      "         [ 0.52719235,  0.52719235,  0.52719235],\n",
      "         [ 0.50985539,  0.50985539,  0.50985539],\n",
      "         ..., \n",
      "         [ 0.14578024,  0.14578024,  0.14578024],\n",
      "         [ 0.18045406,  0.18045406,  0.18045406],\n",
      "         [ 0.18045406,  0.18045406,  0.18045406]],\n",
      "\n",
      "        [[ 0.52719235,  0.52719235,  0.52719235],\n",
      "         [ 0.50985539,  0.50985539,  0.50985539],\n",
      "         [ 0.50985539,  0.50985539,  0.50985539],\n",
      "         ..., \n",
      "         [ 0.12844332,  0.12844332,  0.12844332],\n",
      "         [ 0.18045406,  0.18045406,  0.18045406],\n",
      "         [ 0.19779097,  0.19779097,  0.19779097]]],\n",
      "\n",
      "\n",
      "       [[[ 0.20935763,  0.20935763,  0.20935763],\n",
      "         [ 0.20935763,  0.20935763,  0.20935763],\n",
      "         [ 0.2270332 ,  0.2270332 ,  0.2270332 ],\n",
      "         ..., \n",
      "         [ 0.17400651,  0.17400651,  0.17400651],\n",
      "         [-0.02042469, -0.02042469, -0.02042469],\n",
      "         [-0.12647808, -0.12647808, -0.12647808]],\n",
      "\n",
      "        [[ 0.2270332 ,  0.2270332 ,  0.2270332 ],\n",
      "         [ 0.19168207,  0.19168207,  0.19168207],\n",
      "         [ 0.2270332 ,  0.2270332 ,  0.2270332 ],\n",
      "         ..., \n",
      "         [ 0.2270332 ,  0.2270332 ,  0.2270332 ],\n",
      "         [-0.00274913, -0.00274913, -0.00274913],\n",
      "         [-0.12647808, -0.12647808, -0.12647808]],\n",
      "\n",
      "        [[ 0.17400651,  0.17400651,  0.17400651],\n",
      "         [ 0.17400651,  0.17400651,  0.17400651],\n",
      "         [ 0.13865538,  0.13865538,  0.13865538],\n",
      "         ..., \n",
      "         [ 0.20935763,  0.20935763,  0.20935763],\n",
      "         [-0.02042469, -0.02042469, -0.02042469],\n",
      "         [-0.14415364, -0.14415364, -0.14415364]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.08562869,  0.08562869,  0.08562869],\n",
      "         [ 0.10330425,  0.10330425,  0.10330425],\n",
      "         [ 0.13865538,  0.13865538,  0.13865538],\n",
      "         ..., \n",
      "         [ 1.62340271,  1.62340271,  1.62340271],\n",
      "         [ 1.90621173,  1.90621173,  1.90621173],\n",
      "         [ 1.97691393,  1.97691393,  1.97691393]],\n",
      "\n",
      "        [[ 0.06795312,  0.06795312,  0.06795312],\n",
      "         [ 0.10330425,  0.10330425,  0.10330425],\n",
      "         [ 0.10330425,  0.10330425,  0.10330425],\n",
      "         ..., \n",
      "         [ 1.57037604,  1.57037604,  1.57037604],\n",
      "         [ 1.87086058,  1.87086058,  1.87086058],\n",
      "         [ 1.95923841,  1.95923841,  1.95923841]],\n",
      "\n",
      "        [[ 0.032602  ,  0.032602  ,  0.032602  ],\n",
      "         [ 0.06795312,  0.06795312,  0.06795312],\n",
      "         [ 0.06795312,  0.06795312,  0.06795312],\n",
      "         ..., \n",
      "         [ 1.67642939,  1.67642939,  1.67642939],\n",
      "         [ 1.92388725,  1.92388725,  1.92388725],\n",
      "         [ 1.95923841,  1.95923841,  1.95923841]]],\n",
      "\n",
      "\n",
      "       ..., \n",
      "       [[[-0.02363087, -0.02363087, -0.02363087],\n",
      "         [-0.00857504, -0.00857504, -0.00857504],\n",
      "         [ 0.05164828,  0.05164828,  0.05164828],\n",
      "         ..., \n",
      "         [ 1.16577959,  1.16577959,  1.16577959],\n",
      "         [ 1.57228708,  1.57228708,  1.57228708],\n",
      "         [ 1.24105883,  1.24105883,  1.24105883]],\n",
      "\n",
      "        [[ 0.02153662,  0.02153662,  0.02153662],\n",
      "         [ 0.00648079,  0.00648079,  0.00648079],\n",
      "         [ 0.06670411,  0.06670411,  0.06670411],\n",
      "         ..., \n",
      "         [ 1.22600293,  1.22600293,  1.22600293],\n",
      "         [ 1.49700785,  1.49700785,  1.49700785],\n",
      "         [ 1.16577959,  1.16577959,  1.16577959]],\n",
      "\n",
      "        [[ 0.02153662,  0.02153662,  0.02153662],\n",
      "         [ 0.03659245,  0.03659245,  0.03659245],\n",
      "         [ 0.06670411,  0.06670411,  0.06670411],\n",
      "         ..., \n",
      "         [ 1.40667295,  1.40667295,  1.40667295],\n",
      "         [ 1.49700785,  1.49700785,  1.49700785],\n",
      "         [ 1.12061214,  1.12061214,  1.12061214]],\n",
      "\n",
      "        ..., \n",
      "        [[-0.14407751, -0.14407751, -0.14407751],\n",
      "         [-0.21935666, -0.21935666, -0.21935666],\n",
      "         [-0.27957997, -0.27957997, -0.27957997],\n",
      "         ..., \n",
      "         [ 1.36150539,  1.36150539,  1.36150539],\n",
      "         [ 1.36150539,  1.36150539,  1.36150539],\n",
      "         [ 1.37656128,  1.37656128,  1.37656128]],\n",
      "\n",
      "        [[-0.14407751, -0.14407751, -0.14407751],\n",
      "         [-0.23441248, -0.23441248, -0.23441248],\n",
      "         [-0.2946358 , -0.2946358 , -0.2946358 ],\n",
      "         ..., \n",
      "         [ 1.34644961,  1.34644961,  1.34644961],\n",
      "         [ 1.36150539,  1.36150539,  1.36150539],\n",
      "         [ 1.36150539,  1.36150539,  1.36150539]],\n",
      "\n",
      "        [[-0.15913333, -0.15913333, -0.15913333],\n",
      "         [-0.27957997, -0.27957997, -0.27957997],\n",
      "         [-0.33980328, -0.33980328, -0.33980328],\n",
      "         ..., \n",
      "         [ 1.31633794,  1.31633794,  1.31633794],\n",
      "         [ 1.33139372,  1.33139372,  1.33139372],\n",
      "         [ 1.34644961,  1.34644961,  1.34644961]]],\n",
      "\n",
      "\n",
      "       [[[ 1.09592342,  1.09592342,  1.09592342],\n",
      "         [ 1.0861491 ,  1.0861491 ,  1.0861491 ],\n",
      "         [ 1.09592342,  1.09592342,  1.09592342],\n",
      "         ..., \n",
      "         [ 1.06660068,  1.06660068,  1.06660068],\n",
      "         [ 1.06660068,  1.06660068,  1.06660068],\n",
      "         [ 1.06660068,  1.06660068,  1.06660068]],\n",
      "\n",
      "        [[ 1.09592342,  1.09592342,  1.09592342],\n",
      "         [ 1.0861491 ,  1.0861491 ,  1.0861491 ],\n",
      "         [ 1.0861491 ,  1.0861491 ,  1.0861491 ],\n",
      "         ..., \n",
      "         [ 1.06660068,  1.06660068,  1.06660068],\n",
      "         [ 1.06660068,  1.06660068,  1.06660068],\n",
      "         [ 1.07637489,  1.07637489,  1.07637489]],\n",
      "\n",
      "        [[ 1.0861491 ,  1.0861491 ,  1.0861491 ],\n",
      "         [ 1.09592342,  1.09592342,  1.09592342],\n",
      "         [ 1.09592342,  1.09592342,  1.09592342],\n",
      "         ..., \n",
      "         [ 1.0861491 ,  1.0861491 ,  1.0861491 ],\n",
      "         [ 1.0861491 ,  1.0861491 ,  1.0861491 ],\n",
      "         [ 1.07637489,  1.07637489,  1.07637489]],\n",
      "\n",
      "        ..., \n",
      "        [[ 1.03727782,  1.03727782,  1.03727782],\n",
      "         [ 1.04705215,  1.04705215,  1.04705215],\n",
      "         [ 1.04705215,  1.04705215,  1.04705215],\n",
      "         ..., \n",
      "         [-1.21080172, -1.21080172, -1.21080172],\n",
      "         [-1.24012446, -1.24012446, -1.24012446],\n",
      "         [-1.29876995, -1.29876995, -1.29876995]],\n",
      "\n",
      "        [[ 1.02750361,  1.02750361,  1.02750361],\n",
      "         [ 1.02750361,  1.02750361,  1.02750361],\n",
      "         [ 1.02750361,  1.02750361,  1.02750361],\n",
      "         ..., \n",
      "         [-1.29876995, -1.29876995, -1.29876995],\n",
      "         [-1.30854428, -1.30854428, -1.30854428],\n",
      "         [-1.31831849, -1.31831849, -1.31831849]],\n",
      "\n",
      "        [[ 1.00795507,  1.00795507,  1.00795507],\n",
      "         [ 1.01772928,  1.01772928,  1.01772928],\n",
      "         [ 1.01772928,  1.01772928,  1.01772928],\n",
      "         ..., \n",
      "         [-1.31831849, -1.31831849, -1.31831849],\n",
      "         [-1.33786702, -1.33786702, -1.33786702],\n",
      "         [-1.32809281, -1.32809281, -1.32809281]]],\n",
      "\n",
      "\n",
      "       [[[-0.40925562, -0.40925562, -0.40925562],\n",
      "         [-0.38798285, -0.38798285, -0.38798285],\n",
      "         [-0.32416457, -0.32416457, -0.32416457],\n",
      "         ..., \n",
      "         [ 0.35656384,  0.35656384,  0.35656384],\n",
      "         [ 0.35656384,  0.35656384,  0.35656384],\n",
      "         [ 0.31401831,  0.31401831,  0.31401831]],\n",
      "\n",
      "        [[-0.38798285, -0.38798285, -0.38798285],\n",
      "         [-0.38798285, -0.38798285, -0.38798285],\n",
      "         [-0.32416457, -0.32416457, -0.32416457],\n",
      "         ..., \n",
      "         [ 0.33529109,  0.33529109,  0.33529109],\n",
      "         [ 0.33529109,  0.33529109,  0.33529109],\n",
      "         [ 0.33529109,  0.33529109,  0.33529109]],\n",
      "\n",
      "        [[-0.40925562, -0.40925562, -0.40925562],\n",
      "         [-0.38798285, -0.38798285, -0.38798285],\n",
      "         [-0.34543732, -0.34543732, -0.34543732],\n",
      "         ..., \n",
      "         [ 0.33529109,  0.33529109,  0.33529109],\n",
      "         [ 0.31401831,  0.31401831,  0.31401831],\n",
      "         [ 0.29274556,  0.29274556,  0.29274556]],\n",
      "\n",
      "        ..., \n",
      "        [[-0.55816495, -0.55816495, -0.55816495],\n",
      "         [-0.55816495, -0.55816495, -0.55816495],\n",
      "         [-0.53689218, -0.53689218, -0.53689218],\n",
      "         ..., \n",
      "         [ 1.14365602,  1.14365602,  1.14365602],\n",
      "         [ 1.16492879,  1.16492879,  1.16492879],\n",
      "         [ 1.16492879,  1.16492879,  1.16492879]],\n",
      "\n",
      "        [[-0.53689218, -0.53689218, -0.53689218],\n",
      "         [-0.53689218, -0.53689218, -0.53689218],\n",
      "         [-0.55816495, -0.55816495, -0.55816495],\n",
      "         ..., \n",
      "         [ 1.12238336,  1.12238336,  1.12238336],\n",
      "         [ 1.18620157,  1.18620157,  1.18620157],\n",
      "         [ 1.20747435,  1.20747435,  1.20747435]],\n",
      "\n",
      "        [[-0.53689218, -0.53689218, -0.53689218],\n",
      "         [-0.5156194 , -0.5156194 , -0.5156194 ],\n",
      "         [-0.49434665, -0.49434665, -0.49434665],\n",
      "         ..., \n",
      "         [ 1.16492879,  1.16492879,  1.16492879],\n",
      "         [ 1.18620157,  1.18620157,  1.18620157],\n",
      "         [ 1.22874713,  1.22874713,  1.22874713]]]], dtype=float32), array([ 1995.,  1960.,  1947.,  2004.,  2013.,  1938.,  1979.,  1931.,\n",
      "        1966.,  1994.,  1949.,  1985.,  1978.,  1974.,  1967.,  1972.,\n",
      "        1944.,  1933.,  1952.,  1959.,  2002.,  1996.,  1986.,  1956.,\n",
      "        1996.,  1994.,  1959.,  2009.,  1970.,  1928.,  1948.,  2001.], dtype=float32), array([ 0.02618214,  0.01309107,  0.01282837,  0.01405429,  0.00827496,\n",
      "        0.01654991,  0.00394046,  0.00770578,  0.02132224,  0.01637478,\n",
      "        0.00669877,  0.01011384,  0.00923818,  0.01484238,  0.03660245,\n",
      "        0.00529772,  0.02631349,  0.00718039,  0.00731173,  0.01825744,\n",
      "        0.01711909,  0.01317863,  0.01195271,  0.00634851,  0.01317863,\n",
      "        0.01637478,  0.01825744,  0.01344133,  0.03279335,  0.00538529,\n",
      "        0.00608581,  0.01330998], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "#print(model.summary())\n",
    "print(train.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "num_experiment = 13\n",
    "if training:\n",
    "    with tf.device('/gpu:0'):\n",
    "        filename = \"will_{}.h5\".format(num_experiment)\n",
    "        print (\"Starting experiment \" + str(num_experiment))\n",
    "        model.fit_generator(train, steps_per_epoch = train.steps, epochs = 20,                                \n",
    "                                   validation_data = valid, \n",
    "                                   validation_steps = valid.steps,\n",
    "                                   callbacks=[LearningRateScheduler(lr_schedule),\n",
    "                                ModelCheckpoint(data_path + filename, save_best_only=True)]\n",
    "                           )\n",
    "        print(\"Saved \" + filename)\n",
    "        \n",
    "else:\n",
    "    model.load_weights(data_path + 'will_{}.h5'.format(num_experiment))\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate_generator(valid, valid.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception experiments\n",
    "\n",
    "### Warning: Experiment 6, 7, 8 had bad learning rates\n",
    "\n",
    "will_6.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 16\n",
    "\n",
    "Epoch 8/20\n",
    "713/713 [==============================] - 640s - loss: 6.6840 - mean_absolute_error: 20.0524 - val_loss: 3.9468 - val_mean_absolute_error: 16.9953\n",
    "\n",
    "will_7.h5:\n",
    "- loss = mae\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 16\n",
    "\n",
    "Epoch 3/20\n",
    "713/713 [==============================] - 642s - loss: 0.2597 - mean_absolute_error: 19.9808 - val_loss: 0.1756 - val_mean_absolute_error: 17.1463\n",
    "\n",
    "\n",
    "will_8.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 5/20\n",
    "713/713 [==============================] - 640s - loss: 6.6749 - mean_absolute_error: 20.0370 - val_loss: 3.9561 - val_mean_absolute_error: 16.9929\n",
    "\n",
    "\n",
    "will_9.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "713/713 [==============================] - 640s - loss: 0.0052 - mean_absolute_error: 0.5249 - val_loss: 0.2931 - val_mean_absolute_error: 3.6388\n",
    "\n",
    "will_10.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "713/713 [==============================] - 648s - loss: 0.0275 - mean_absolute_error: 1.1997 - val_loss: 0.2914 - val_mean_absolute_error: 3.5716\n",
    "\n",
    "will_11.h5:\n",
    "- loss = mse\n",
    "- epochs = 6 wita data augmentation * 3\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "2139/2139 [==============================] - 1847s - loss: 0.0788 - mean_absolute_error: 1.8316 - val_loss: 0.3522 - val_mean_absolute_error: 3.9830\n",
    "\n",
    "will_12.h5:\n",
    "- loss = mae\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "713/713 [==============================] - 635s - loss: 0.0057 - mean_absolute_error: 0.6751 - val_loss: 0.0363 - val_mean_absolute_error: 3.4857\n",
    "\n",
    "will_13.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 8 filers\n",
    "- dense = 16\n",
    "\n",
    "713/713 [==============================] - 640s - loss: 0.0076 - mean_absolute_error: 0.6360 - val_loss: 0.2842 - val_mean_absolute_error: 3.5779\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19 experiments\n",
    "\n",
    "will_1.h5:\n",
    "- loss = mse\n",
    "- epochs = 3\n",
    "- lr = 1e-2\n",
    "- conv 1x1 = 64 filers\n",
    "- dense = 128\n",
    "\n",
    "Epoch 1/3\n",
    "2139/2139 [==============================] - 1777s - loss: 1.7294 - acc: 0.0433 - mean_absolute_error: 8.9744 - val_loss: 1.2350 - val_acc: 0.0403 - val_mean_absolute_error: 8.3663\n",
    "\n",
    "Epoch 2/3\n",
    "2139/2139 [==============================] - 1706s - loss: 1.0568 - acc: 0.0573 - mean_absolute_error: 6.9857 - val_loss: 1.3283 - val_acc: 0.0404 - val_mean_absolute_error: 8.6860\n",
    "\n",
    "Epoch 3/3\n",
    "2139/2139 [==============================] - 1704s - loss: 0.8709 - acc: 0.0644 - mean_absolute_error: 6.3560 - val_loss: 0.9722 - val_acc: 0.0547 - **val_mean_absolute_error: 7.2192**\n",
    "\n",
    "will_2.h5:\n",
    "- loss = mse\n",
    "- epochs = 1\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 1/1\n",
    "2139/2139 [==============================] - 1718s - loss: 1.9574 - acc: 0.0403 - mean_absolute_error: 9.6086 - val_loss: 1.6574 - val_acc: 0.0375 - **val_mean_absolute_error: 9.5908**\n",
    "\n",
    "will_3.h5:\n",
    "- loss = mse\n",
    "- epochs = 3 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 1/3\n",
    "713/713 [==============================] - 389s - loss: 2.4174 - acc: 0.0371 - mean_absolute_error: 10.8405 - val_loss: 1.3939 - val_acc: 0.0381 - val_mean_absolute_error: 9.0965\n",
    "\n",
    "Epoch 2/3\n",
    "713/713 [==============================] - 388s - loss: 1.8113 - acc: 0.0436 - mean_absolute_error: 9.3031 - val_loss: 1.4613 - val_acc: 0.0378 - **val_mean_absolute_error: 9.0060**\n",
    "\n",
    "Epoch 3/3\n",
    "713/713 [==============================] - 385s - loss: 1.6857 - acc: 0.0430 - mean_absolute_error: 8.9575 - val_loss: 1.8654 - val_acc: 0.0237 - val_mean_absolute_error: 10.2079\n",
    "\n",
    "will_4.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 50 filers\n",
    "- dense = 100\n",
    "\n",
    "Epoch 1/20\n",
    "713/713 [==============================] - 394s - loss: 2.4342 - acc: 0.0356 - mean_absolute_error: 10.8500 - val_loss: 2.6718 - val_acc: 0.0294 - val_mean_absolute_error: 12.2755\n",
    "\n",
    "Epoch 2/20\n",
    "713/713 [==============================] - 393s - loss: 1.7797 - acc: 0.0438 - mean_absolute_error: 9.2208 - val_loss: 1.3407 - val_acc: 0.0317 - val_mean_absolute_error: 8.7575\n",
    "\n",
    "Epoch 3/20\n",
    "713/713 [==============================] - 389s - loss: 1.6081 - acc: 0.0499 - mean_absolute_error: 8.7490 - val_loss: 1.2787 - val_acc: 0.0364 - val_mean_absolute_error: 8.6355\n",
    "\n",
    "Epoch 4/20\n",
    "713/713 [==============================] - 389s - loss: 1.4956 - acc: 0.0526 - mean_absolute_error: 8.3522 - val_loss: 1.2091 - val_acc: 0.0358 - val_mean_absolute_error: 8.1282\n",
    "\n",
    "Epoch 5/20\n",
    "713/713 [==============================] - 389s - loss: 1.3725 - acc: 0.0514 - mean_absolute_error: 8.0273 - val_loss: 1.1532 - val_acc: 0.0484 - val_mean_absolute_error: 7.7425\n",
    "\n",
    "Epoch 6/20\n",
    "713/713 [==============================] - 389s - loss: 1.2938 - acc: 0.0488 - mean_absolute_error: 7.8258 - val_loss: 1.0986 - val_acc: 0.0400 - val_mean_absolute_error: 7.7927\n",
    "\n",
    "Epoch 7/20\n",
    "713/713 [==============================] - 388s - loss: 1.2713 - acc: 0.0480 - mean_absolute_error: 7.7210 - val_loss: 1.1828 - val_acc: 0.0410 - val_mean_absolute_error: 8.0945\n",
    "\n",
    "Epoch 8/20\n",
    "713/713 [==============================] - 388s - loss: 1.2350 - acc: 0.0521 - mean_absolute_error: 7.6468 - val_loss: 1.5489 - val_acc: 0.0342 - val_mean_absolute_error: 9.0588\n",
    "\n",
    "Epoch 9/20\n",
    "713/713 [==============================] - 387s - loss: 1.2230 - acc: 0.0492 - mean_absolute_error: 7.5878 - val_loss: 1.1071 - val_acc: 0.0430 - val_mean_absolute_error: 7.7185\n",
    "Epoch 10/20\n",
    "\n",
    "713/713 [==============================] - 388s - loss: 1.1237 - acc: 0.0517 - mean_absolute_error: 7.2646 - val_loss: 1.3911 - val_acc: 0.0362 - val_mean_absolute_error: 8.8584\n",
    "\n",
    "Epoch 11/20\n",
    "713/713 [==============================] - 389s - loss: 0.9102 - acc: 0.0594 - mean_absolute_error: 6.4532 - val_loss: 0.9581 - val_acc: 0.0486 - val_mean_absolute_error: 7.0721\n",
    "\n",
    "Epoch 12/20\n",
    "713/713 [==============================] - 389s - loss: 0.8178 - acc: 0.0612 - mean_absolute_error: 6.1367 - val_loss: 0.9202 - val_acc: 0.0442 - val_mean_absolute_error: 7.1384\n",
    "\n",
    "Epoch 13/20\n",
    "713/713 [==============================] - 387s - loss: 0.8100 - acc: 0.0622 - mean_absolute_error: 6.1223 - val_loss: 0.9951 - val_acc: 0.0444 - val_mean_absolute_error: 7.2625\n",
    "\n",
    "Epoch 14/20\n",
    "713/713 [==============================] - 387s - loss: 0.8052 - acc: 0.0627 - mean_absolute_error: 6.1083 - val_loss: 0.9653 - val_acc: 0.0524 - val_mean_absolute_error: 7.1744\n",
    "\n",
    "Epoch 15/20\n",
    "713/713 [==============================] - 389s - loss: 0.7664 - acc: 0.0652 - mean_absolute_error: 5.9908 - val_loss: 0.9191 - val_acc: 0.0478 - val_mean_absolute_error: 7.0468\n",
    "\n",
    "Epoch 16/20\n",
    "713/713 [==============================] - 387s - loss: 0.7592 - acc: 0.0688 - mean_absolute_error: 5.8906 - val_loss: 0.9926 - val_acc: 0.0464 - val_mean_absolute_error: 7.1721\n",
    "\n",
    "Epoch 17/20\n",
    "713/713 [==============================] - 387s - loss: 0.7411 - acc: 0.0644 - mean_absolute_error: 5.8423 - val_loss: 0.9241 - val_acc: 0.0438 - val_mean_absolute_error: 7.1005\n",
    "\n",
    "Epoch 18/20\n",
    "713/713 [==============================] - 388s - loss: 0.7309 - acc: 0.0586 - mean_absolute_error: 5.8033 - val_loss: 0.9584 - val_acc: 0.0661 - val_mean_absolute_error: 7.0660\n",
    "\n",
    "Epoch 19/20\n",
    "713/713 [==============================] - 387s - loss: 0.7039 - acc: 0.0581 - mean_absolute_error: 5.7438 - val_loss: 0.9351 - val_acc: 0.0663 - **val_mean_absolute_error: 6.9641**\n",
    "\n",
    "Epoch 20/20\n",
    "713/713 [==============================] - 388s - loss: 0.6950 - acc: 0.0572 - mean_absolute_error: 5.6644 - val_loss: 0.9598 - val_acc: 0.0607 - val_mean_absolute_error: 7.1322\n",
    "\n",
    "will_5.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 50 filers\n",
    "- dense = 100\n",
    "- Dropout = 0.5\n",
    "\n",
    "Epoch 1/20\n",
    "713/713 [==============================] - 392s - loss: 3.2925 - acc: 0.0291 - mean_absolute_error: 12.9005 - val_loss: 1.6876 - val_acc: 0.0294 - val_mean_absolute_error: 10.0642\n",
    "\n",
    "Epoch 2/20\n",
    "713/713 [==============================] - 391s - loss: 2.6793 - acc: 0.0331 - mean_absolute_error: 11.5035 - val_loss: 1.3901 - val_acc: 0.0388 - val_mean_absolute_error: 8.8441\n",
    "\n",
    "Epoch 3/20\n",
    "713/713 [==============================] - 385s - loss: 2.5866 - acc: 0.0285 - mean_absolute_error: 11.2949 - val_loss: 3.0742 - val_acc: 0.0189 - val_mean_absolute_error: 13.7363\n",
    "\n",
    "Epoch 4/20\n",
    "713/713 [==============================] - 386s - loss: 2.5862 - acc: 0.0292 - mean_absolute_error: 11.3336 - val_loss: 1.9153 - val_acc: 0.0334 - val_mean_absolute_error: 9.7895\n",
    "\n",
    "Epoch 5/20\n",
    "713/713 [==============================] - 386s - loss: 2.6258 - acc: 0.0278 - mean_absolute_error: 11.3908 - val_loss: 2.3101 - val_acc: 0.0185 - val_mean_absolute_error: 11.5918\n",
    "\n",
    "Epoch 6/20\n",
    "713/713 [==============================] - 386s - loss: 2.5469 - acc: 0.0292 - mean_absolute_error: 11.2433 - val_loss: 7.3051 - val_acc: 0.0384 - val_mean_absolute_error: 10.0754\n",
    "\n",
    "Epoch 7/20\n",
    "713/713 [==============================] - 385s - loss: 2.5583 - acc: 0.0292 - mean_absolute_error: 11.2443 - val_loss: 6.9639 - val_acc: 0.0416 - val_mean_absolute_error: 10.8122\n",
    "\n",
    "Epoch 8/20\n",
    "713/713 [==============================] - 386s - loss: 2.5845 - acc: 0.0273 - mean_absolute_error: 11.3074 - val_loss: 7.7042 - val_acc: 0.0330 - val_mean_absolute_error: 11.1933\n",
    "\n",
    "Epoch 9/20\n",
    "713/713 [==============================] - 385s - loss: 2.5135 - acc: 0.0310 - mean_absolute_error: 11.1403 - val_loss: 6.8166 - val_acc: 0.0384 - val_mean_absolute_error: 11.4688\n",
    "\n",
    "Epoch 10/20\n",
    "713/713 [==============================] - 386s - loss: 2.3537 - acc: 0.0281 - mean_absolute_error: 10.8007 - val_loss: 7.8546 - val_acc: 0.0358 - val_mean_absolute_error: 11.1792\n",
    "\n",
    "Epoch 11/20\n",
    "713/713 [==============================] - 385s - loss: 2.2141 - acc: 0.0310 - mean_absolute_error: 10.4251 - val_loss: 6.0965 - val_acc: 0.0382 - val_mean_absolute_error: 9.4952\n",
    "\n",
    "Epoch 12/20\n",
    "713/713 [==============================] - 385s - loss: 2.1059 - acc: 0.0326 - mean_absolute_error: 10.1197 - val_loss: 11.5091 - val_acc: 0.0446 - val_mean_absolute_error: 10.5134\n",
    "\n",
    "Epoch 13/20\n",
    "713/713 [==============================] - 386s - loss: 2.0857 - acc: 0.0334 - mean_absolute_error: 10.0760 - val_loss: 12.5309 - val_acc: 0.0436 - val_mean_absolute_error: 11.5219\n",
    "\n",
    "Epoch 14/20\n",
    "713/713 [==============================] - 386s - loss: 2.0577 - acc: 0.0334 - mean_absolute_error: 10.0021 - val_loss: 15.5565 - val_acc: 0.0400 - val_mean_absolute_error: 11.1698\n",
    "\n",
    "Epoch 15/20\n",
    "713/713 [==============================] - 386s - loss: 2.0236 - acc: 0.0343 - mean_absolute_error: 9.8573 - val_loss: 5.3131 - val_acc: 0.0388 - val_mean_absolute_error: 9.0751\n",
    "\n",
    "Epoch 16/20\n",
    "713/713 [==============================] - 386s - loss: 1.9644 - acc: 0.0344 - mean_absolute_error: 9.7395 - val_loss: 8.4828 - val_acc: 0.0334 - val_mean_absolute_error: 10.6123\n",
    "\n",
    "Epoch 17/20\n",
    "713/713 [==============================] - 386s - loss: 1.9953 - acc: 0.0345 - mean_absolute_error: 9.7768 - val_loss: 6.1084 - val_acc: 0.0378 - val_mean_absolute_error: 9.5485\n",
    "\n",
    "Epoch 18/20\n",
    "713/713 [==============================] - 385s - loss: 1.9610 - acc: 0.0351 - mean_absolute_error: 9.6949 - val_loss: 7.6045 - val_acc: 0.0382 - val_mean_absolute_error: 9.7756\n",
    "\n",
    "Epoch 19/20\n",
    "713/713 [==============================] - 386s - loss: 1.9285 - acc: 0.0355 - mean_absolute_error: 9.6024 - val_loss: 4.7563 - val_acc: 0.0372 - val_mean_absolute_error: 9.2489\n",
    "\n",
    "Epoch 20/20\n",
    "713/713 [==============================] - 385s - loss: 1.9519 - acc: 0.0358 - mean_absolute_error: 9.6799 - val_loss: 4.6270 - val_acc: 0.0444 - **val_mean_absolute_error: 9.0526**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
