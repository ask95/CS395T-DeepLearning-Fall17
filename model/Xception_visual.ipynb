{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Dropout, Flatten, Activation, Lambda, Input\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Lambda\n",
    "from keras import initializers\n",
    "from keras.utils import get_file\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "data_path = '/work/04381/ymarathe/maverick/yearbook/'\n",
    "path = '/home/05145/anikeshk/CS395T-DeepLearning-Fall17/model/'\n",
    "\n",
    "f = open(data_path + 'yearbook_train.txt', 'r')\n",
    "\n",
    "freq = {};\n",
    "normal_const = 0;\n",
    "\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    image, year = line.split(\"\\t\")\n",
    "    if year in freq:\n",
    "        freq[year] += 1\n",
    "    else:\n",
    "        freq[year] = 1\n",
    "\n",
    "normal_const = np.sum(freq.values())\n",
    "for key in freq:\n",
    "    freq[key] = freq[key]/float(normal_const);\n",
    "    \n",
    "sorted_freq = collections.OrderedDict(sorted(freq.items()))\n",
    "\n",
    "idx = 0;\n",
    "class_weights_train = {}\n",
    "idx2year = {}\n",
    "\n",
    "for key in sorted_freq:\n",
    "    class_weights_train[idx] = sorted_freq[key]\n",
    "    idx2year[idx] = key\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.image import apply_transform, transform_matrix_offset_center\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import random_rotation, random_shear, random_shift, random_zoom\n",
    "from skimage import exposure\n",
    "\n",
    "class RegressDataGen:\n",
    "    def __init__(self, directory, map_file, target_size = (171, 186, 3), \n",
    "                 class_weights_train = None, multi_output=False, do_augmentation=True, \n",
    "                 samplewise_center = True,\n",
    "                 samplewise_std_deviation = True,\n",
    "                 multi_input=False\n",
    "                ):\n",
    "        self.directory = directory\n",
    "        self.map_file = map_file\n",
    "        self.filenames = []\n",
    "        self.map = {}\n",
    "        self.fnameToGender = {}\n",
    "        self.target_size = target_size\n",
    "        self.populate_filenames()\n",
    "        self.populate_mapping()\n",
    "        self.regressIter = None\n",
    "        self.steps = 0\n",
    "        self.samplewise_center = samplewise_center\n",
    "        self.samplewise_std_deviation = samplewise_std_deviation\n",
    "        self.height_shift_range = 0.2\n",
    "        self.width_shift_range = 0.2\n",
    "        self.max_rotation = 45\n",
    "        self.shear = 0.785398\n",
    "        self.zoom_range = (0.5, 0.5)\n",
    "        self.do_augmentation = do_augmentation\n",
    "        self.class_weights_train = class_weights_train\n",
    "        self.equalizehist = False\n",
    "        self.multi_output = multi_output\n",
    "        self.multi_input = multi_input\n",
    "        self.lastN = []\n",
    "        \n",
    "    def _recursive_list(self, subpath):\n",
    "        return sorted(\n",
    "            os.walk(subpath, followlinks=False), key=lambda tpl: tpl[0])\n",
    "    \n",
    "    def populate_mapping(self):\n",
    "        f = open(self.map_file, 'r')\n",
    "\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            image, year = line.split(\"\\t\")\n",
    "            gender, imfilename = image.split(\"/\")\n",
    "            if gender is 'M':\n",
    "                encodeGender = 1\n",
    "            elif gender is 'F':\n",
    "                encodeGender = 0\n",
    "            self.fnameToGender[image] = encodeGender\n",
    "            self.map[image] = year\n",
    "            \n",
    "    def populate_filenames(self):\n",
    "        base_dir = self.directory\n",
    "        for root, _, files in self._recursive_list(base_dir):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith('.' + 'png'):\n",
    "                    self.filenames.append(os.path.relpath(os.path.join(root, fname), base_dir))\n",
    "                    \n",
    "    def preprocess(self, x):\n",
    "        if self.equalizehist:\n",
    "            print(x)\n",
    "            x = exposure.equalize_hist(x)\n",
    "            print(x)\n",
    "            exit()\n",
    "        return x\n",
    "            \n",
    "    def augment_data(self, x):\n",
    "        \n",
    "        x = random_shift(x, self.width_shift_range, self.height_shift_range, \n",
    "                         row_axis=0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_rotation(x, self.max_rotation, \n",
    "                            row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_shear(x, self.shear, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_zoom(x, self.zoom_range, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "    def flow_from_directory(self, batch_size = 32, shuffle = True, seed = 42):\n",
    "        \n",
    "        self.regressIter = Iterator(len(self.filenames), batch_size = batch_size, shuffle = shuffle, seed = seed)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            factor = 3\n",
    "        else:\n",
    "            factor = 1\n",
    "        \n",
    "        self.steps = math.ceil(len(self.filenames)/batch_size) * factor\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def next(self, *args, **kwargs):\n",
    "           \n",
    "        self.lastN = []\n",
    "        \n",
    "        idx_array, cur_idx, bs = next(self.regressIter.index_generator)\n",
    "        \n",
    "        batch_x = np.zeros(tuple([len(idx_array)] + list(self.target_size)), dtype=K.floatx())\n",
    "        \n",
    "        batch_y = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        if self.multi_output:\n",
    "            batch_y_gender = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "    \n",
    "        if self.multi_input:\n",
    "            batch_x_gender = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        if self.class_weights_train is not None:\n",
    "            sample_weights = np.ones(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        for i, j in enumerate(idx_array):\n",
    "            fname = self.filenames[j]\n",
    "            self.lastN.append(fname)\n",
    "            img = load_img(\n",
    "                  os.path.join(self.directory, fname),\n",
    "                  grayscale = True,\n",
    "                  target_size= self.target_size)\n",
    "            x = np.array(img_to_array(img, data_format='channels_last'))\n",
    "            x = self.preprocess(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = self.map[fname]\n",
    "            \n",
    "            if self.multi_output:\n",
    "                batch_y_gender[i] = self.fnameToGender[fname]\n",
    "            \n",
    "            if self.multi_input:\n",
    "                batch_x_gender[i] = self.fnameToGender[fname]\n",
    "            \n",
    "            if self.class_weights_train is not None:\n",
    "                if self.multi_output:\n",
    "                    sample_weights[i] = self.class_weights_train[batch_y[i].astype('int').astype('str')]\n",
    "                else:\n",
    "                    sample_weights[i] = self.class_weights_train[batch_y[i].astype('int').astype('str')]\n",
    "        \n",
    "        if self.samplewise_center:\n",
    "            for x in batch_x:\n",
    "                x -= np.mean(x)\n",
    "        \n",
    "        if self.samplewise_std_deviation:\n",
    "            for x in batch_x:\n",
    "                x /= np.std(x)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            for x in batch_x:\n",
    "                x = self.augment_data(x)\n",
    "        \n",
    "        if self.multi_output:\n",
    "            if self.class_weights_train is not None:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}, {'out_year' : sample_weights, 'out_gender' : sample_weights} \n",
    "            else:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}\n",
    "            \n",
    "        elif self.multi_input:\n",
    "            if self.class_weights_train is not None:\n",
    "                return {'input_1' : batch_x, 'input_2': batch_x_gender}, batch_y, sample_weights\n",
    "            else:\n",
    "                return {'input_1' : batch_x, 'input_2': batch_x_gender}, batch_y\n",
    "        else:    \n",
    "            if self.class_weights_train is not None:\n",
    "                return (batch_x, batch_y, sample_weights)\n",
    "            else:\n",
    "                return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "train = RegressDataGen(data_path + 'train',\n",
    "                       data_path + 'yearbook_train.txt', \n",
    "                       class_weights_train = sorted_freq,\n",
    "                       do_augmentation = False,\n",
    "                      )\n",
    "valid = RegressDataGen(data_path + 'valid',\n",
    "                       data_path + 'yearbook_valid.txt',\n",
    "                       class_weights_train = sorted_freq, \n",
    "                       do_augmentation = False,\n",
    "                      )\n",
    "\n",
    "train = train.flow_from_directory()\n",
    "valid = valid.flow_from_directory(shuffle=False)\n",
    "\n",
    "mean_value = 0\n",
    "for key in freq:\n",
    "    mean_value += freq[key] * float(key)\n",
    "\n",
    "\n",
    "pretrained_model = Xception(include_top=False, weights='imagenet', input_shape=(171, 186, 3))\n",
    "x = pretrained_model.output\n",
    "x = Conv2D(8, (1, 1), activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation='relu', bias_initializer=keras.initializers.Ones())(x)\n",
    "#x = BatchNormalization()(x)\n",
    "predicted_year = Dense(1, bias_initializer = keras.initializers.Constant(mean_value))(x)\n",
    "\n",
    "model = Model(inputs=pretrained_model.input, outputs=predicted_year)\n",
    "\n",
    "lr = 1e-3\n",
    "def lr_schedule(epoch):\n",
    "    return lr * (0.1 ** float(epoch / 10.0))\n",
    "\n",
    "model.compile(Adam(lr=lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = False\n",
    "num_experiment = 15\n",
    "import numpy as np\n",
    "from scipy.misc import imsave\n",
    "\n",
    "if training:\n",
    "    with tf.device('/gpu:0'):\n",
    "        filename = \"will_{}.h5\".format(num_experiment)\n",
    "        print (\"Starting experiment \" + str(num_experiment))\n",
    "        model.fit_generator(train, steps_per_epoch = train.steps, epochs = 20,                                \n",
    "                                   validation_data = valid, \n",
    "                                   validation_steps = valid.steps,\n",
    "                                   callbacks=[LearningRateScheduler(lr_schedule),\n",
    "                                ModelCheckpoint(path + filename, save_best_only=True)]\n",
    "                           )\n",
    "        print(\"Saved \" + filename)\n",
    "        \n",
    "else:\n",
    "    #model.load_weights('/home/05148/picsou/project1/data/yearbook/' + 'will_{}.h5'.format(num_experiment))\n",
    "    input_img = model.input\n",
    "    \n",
    "    print(len(model.layers))\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer_name = 'block14_sepconv2'\n",
    "    \n",
    "    filter_idx = 3\n",
    "    layer_dict[layer_name].output\n",
    "    loss = K.mean(layer_output[:,:,:,filter_idx])\n",
    "    \n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "    \n",
    "    \n",
    "\n",
    "# we start from a gray image with some noise\n",
    "    input_img_data = \n",
    "# run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        \n",
    "    \n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "    def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "        x -= x.mean()\n",
    "        x /= (x.std() + 1e-5)\n",
    "        x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "        x += 0.5\n",
    "        x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "        x *= 255\n",
    "        x = x.transpose((1, 2, 0))\n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "        return x\n",
    "\n",
    "    img = input_img_data[0]\n",
    "    img = deprocess_image(img)\n",
    "    imsave('%s_filter_%d.png' % (layer_name, filter_index), img)\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(model.metrics_names)\n",
    "    #print(\"Model loaded\")\n",
    "    #print(model.evaluate_generator(valid, valid.steps))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception experiments\n",
    "\n",
    "### Warning: Experiment 6, 7, 8 had bad learning rates\n",
    "\n",
    "will_6.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 16\n",
    "\n",
    "Epoch 8/20\n",
    "713/713 [==============================] - 640s - loss: 6.6840 - mean_absolute_error: 20.0524 - val_loss: 3.9468 - val_mean_absolute_error: 16.9953\n",
    "\n",
    "will_7.h5:\n",
    "- loss = mae\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 16\n",
    "\n",
    "Epoch 3/20\n",
    "713/713 [==============================] - 642s - loss: 0.2597 - mean_absolute_error: 19.9808 - val_loss: 0.1756 - val_mean_absolute_error: 17.1463\n",
    "\n",
    "\n",
    "will_8.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 5/20\n",
    "713/713 [==============================] - 640s - loss: 6.6749 - mean_absolute_error: 20.0370 - val_loss: 3.9561 - val_mean_absolute_error: 16.9929\n",
    "\n",
    "\n",
    "will_9.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "713/713 [==============================] - 640s - loss: 0.0052 - mean_absolute_error: 0.5249 - val_loss: 0.2931 - val_mean_absolute_error: 3.6388\n",
    "\n",
    "will_10.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "713/713 [==============================] - 648s - loss: 0.0275 - mean_absolute_error: 1.1997 - val_loss: 0.2914 - val_mean_absolute_error: 3.5716\n",
    "\n",
    "will_11.h5:\n",
    "- loss = mse\n",
    "- epochs = 6 wita data augmentation * 3\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "2139/2139 [==============================] - 1847s - loss: 0.0788 - mean_absolute_error: 1.8316 - val_loss: 0.3522 - val_mean_absolute_error: 3.9830\n",
    "\n",
    "will_12.h5:\n",
    "- loss = mae\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "713/713 [==============================] - 635s - loss: 0.0057 - mean_absolute_error: 0.6751 - val_loss: 0.0363 - val_mean_absolute_error: 3.4857\n",
    "\n",
    "will_13.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 8 filers\n",
    "- dense = 16\n",
    "\n",
    "713/713 [==============================] - 640s - loss: 0.0076 - mean_absolute_error: 0.6360 - val_loss: 0.2842 - val_mean_absolute_error: 3.5779\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19 experiments\n",
    "\n",
    "will_1.h5:\n",
    "- loss = mse\n",
    "- epochs = 3\n",
    "- lr = 1e-2\n",
    "- conv 1x1 = 64 filers\n",
    "- dense = 128\n",
    "\n",
    "Epoch 1/3\n",
    "2139/2139 [==============================] - 1777s - loss: 1.7294 - acc: 0.0433 - mean_absolute_error: 8.9744 - val_loss: 1.2350 - val_acc: 0.0403 - val_mean_absolute_error: 8.3663\n",
    "\n",
    "Epoch 2/3\n",
    "2139/2139 [==============================] - 1706s - loss: 1.0568 - acc: 0.0573 - mean_absolute_error: 6.9857 - val_loss: 1.3283 - val_acc: 0.0404 - val_mean_absolute_error: 8.6860\n",
    "\n",
    "Epoch 3/3\n",
    "2139/2139 [==============================] - 1704s - loss: 0.8709 - acc: 0.0644 - mean_absolute_error: 6.3560 - val_loss: 0.9722 - val_acc: 0.0547 - **val_mean_absolute_error: 7.2192**\n",
    "\n",
    "will_2.h5:\n",
    "- loss = mse\n",
    "- epochs = 1\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 1/1\n",
    "2139/2139 [==============================] - 1718s - loss: 1.9574 - acc: 0.0403 - mean_absolute_error: 9.6086 - val_loss: 1.6574 - val_acc: 0.0375 - **val_mean_absolute_error: 9.5908**\n",
    "\n",
    "will_3.h5:\n",
    "- loss = mse\n",
    "- epochs = 3 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 1/3\n",
    "713/713 [==============================] - 389s - loss: 2.4174 - acc: 0.0371 - mean_absolute_error: 10.8405 - val_loss: 1.3939 - val_acc: 0.0381 - val_mean_absolute_error: 9.0965\n",
    "\n",
    "Epoch 2/3\n",
    "713/713 [==============================] - 388s - loss: 1.8113 - acc: 0.0436 - mean_absolute_error: 9.3031 - val_loss: 1.4613 - val_acc: 0.0378 - **val_mean_absolute_error: 9.0060**\n",
    "\n",
    "Epoch 3/3\n",
    "713/713 [==============================] - 385s - loss: 1.6857 - acc: 0.0430 - mean_absolute_error: 8.9575 - val_loss: 1.8654 - val_acc: 0.0237 - val_mean_absolute_error: 10.2079\n",
    "\n",
    "will_4.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 50 filers\n",
    "- dense = 100\n",
    "\n",
    "Epoch 1/20\n",
    "713/713 [==============================] - 394s - loss: 2.4342 - acc: 0.0356 - mean_absolute_error: 10.8500 - val_loss: 2.6718 - val_acc: 0.0294 - val_mean_absolute_error: 12.2755\n",
    "\n",
    "Epoch 2/20\n",
    "713/713 [==============================] - 393s - loss: 1.7797 - acc: 0.0438 - mean_absolute_error: 9.2208 - val_loss: 1.3407 - val_acc: 0.0317 - val_mean_absolute_error: 8.7575\n",
    "\n",
    "Epoch 3/20\n",
    "713/713 [==============================] - 389s - loss: 1.6081 - acc: 0.0499 - mean_absolute_error: 8.7490 - val_loss: 1.2787 - val_acc: 0.0364 - val_mean_absolute_error: 8.6355\n",
    "\n",
    "Epoch 4/20\n",
    "713/713 [==============================] - 389s - loss: 1.4956 - acc: 0.0526 - mean_absolute_error: 8.3522 - val_loss: 1.2091 - val_acc: 0.0358 - val_mean_absolute_error: 8.1282\n",
    "\n",
    "Epoch 5/20\n",
    "713/713 [==============================] - 389s - loss: 1.3725 - acc: 0.0514 - mean_absolute_error: 8.0273 - val_loss: 1.1532 - val_acc: 0.0484 - val_mean_absolute_error: 7.7425\n",
    "\n",
    "Epoch 6/20\n",
    "713/713 [==============================] - 389s - loss: 1.2938 - acc: 0.0488 - mean_absolute_error: 7.8258 - val_loss: 1.0986 - val_acc: 0.0400 - val_mean_absolute_error: 7.7927\n",
    "\n",
    "Epoch 7/20\n",
    "713/713 [==============================] - 388s - loss: 1.2713 - acc: 0.0480 - mean_absolute_error: 7.7210 - val_loss: 1.1828 - val_acc: 0.0410 - val_mean_absolute_error: 8.0945\n",
    "\n",
    "Epoch 8/20\n",
    "713/713 [==============================] - 388s - loss: 1.2350 - acc: 0.0521 - mean_absolute_error: 7.6468 - val_loss: 1.5489 - val_acc: 0.0342 - val_mean_absolute_error: 9.0588\n",
    "\n",
    "Epoch 9/20\n",
    "713/713 [==============================] - 387s - loss: 1.2230 - acc: 0.0492 - mean_absolute_error: 7.5878 - val_loss: 1.1071 - val_acc: 0.0430 - val_mean_absolute_error: 7.7185\n",
    "Epoch 10/20\n",
    "\n",
    "713/713 [==============================] - 388s - loss: 1.1237 - acc: 0.0517 - mean_absolute_error: 7.2646 - val_loss: 1.3911 - val_acc: 0.0362 - val_mean_absolute_error: 8.8584\n",
    "\n",
    "Epoch 11/20\n",
    "713/713 [==============================] - 389s - loss: 0.9102 - acc: 0.0594 - mean_absolute_error: 6.4532 - val_loss: 0.9581 - val_acc: 0.0486 - val_mean_absolute_error: 7.0721\n",
    "\n",
    "Epoch 12/20\n",
    "713/713 [==============================] - 389s - loss: 0.8178 - acc: 0.0612 - mean_absolute_error: 6.1367 - val_loss: 0.9202 - val_acc: 0.0442 - val_mean_absolute_error: 7.1384\n",
    "\n",
    "Epoch 13/20\n",
    "713/713 [==============================] - 387s - loss: 0.8100 - acc: 0.0622 - mean_absolute_error: 6.1223 - val_loss: 0.9951 - val_acc: 0.0444 - val_mean_absolute_error: 7.2625\n",
    "\n",
    "Epoch 14/20\n",
    "713/713 [==============================] - 387s - loss: 0.8052 - acc: 0.0627 - mean_absolute_error: 6.1083 - val_loss: 0.9653 - val_acc: 0.0524 - val_mean_absolute_error: 7.1744\n",
    "\n",
    "Epoch 15/20\n",
    "713/713 [==============================] - 389s - loss: 0.7664 - acc: 0.0652 - mean_absolute_error: 5.9908 - val_loss: 0.9191 - val_acc: 0.0478 - val_mean_absolute_error: 7.0468\n",
    "\n",
    "Epoch 16/20\n",
    "713/713 [==============================] - 387s - loss: 0.7592 - acc: 0.0688 - mean_absolute_error: 5.8906 - val_loss: 0.9926 - val_acc: 0.0464 - val_mean_absolute_error: 7.1721\n",
    "\n",
    "Epoch 17/20\n",
    "713/713 [==============================] - 387s - loss: 0.7411 - acc: 0.0644 - mean_absolute_error: 5.8423 - val_loss: 0.9241 - val_acc: 0.0438 - val_mean_absolute_error: 7.1005\n",
    "\n",
    "Epoch 18/20\n",
    "713/713 [==============================] - 388s - loss: 0.7309 - acc: 0.0586 - mean_absolute_error: 5.8033 - val_loss: 0.9584 - val_acc: 0.0661 - val_mean_absolute_error: 7.0660\n",
    "\n",
    "Epoch 19/20\n",
    "713/713 [==============================] - 387s - loss: 0.7039 - acc: 0.0581 - mean_absolute_error: 5.7438 - val_loss: 0.9351 - val_acc: 0.0663 - **val_mean_absolute_error: 6.9641**\n",
    "\n",
    "Epoch 20/20\n",
    "713/713 [==============================] - 388s - loss: 0.6950 - acc: 0.0572 - mean_absolute_error: 5.6644 - val_loss: 0.9598 - val_acc: 0.0607 - val_mean_absolute_error: 7.1322\n",
    "\n",
    "will_5.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 50 filers\n",
    "- dense = 100\n",
    "- Dropout = 0.5\n",
    "\n",
    "Epoch 1/20\n",
    "713/713 [==============================] - 392s - loss: 3.2925 - acc: 0.0291 - mean_absolute_error: 12.9005 - val_loss: 1.6876 - val_acc: 0.0294 - val_mean_absolute_error: 10.0642\n",
    "\n",
    "Epoch 2/20\n",
    "713/713 [==============================] - 391s - loss: 2.6793 - acc: 0.0331 - mean_absolute_error: 11.5035 - val_loss: 1.3901 - val_acc: 0.0388 - val_mean_absolute_error: 8.8441\n",
    "\n",
    "Epoch 3/20\n",
    "713/713 [==============================] - 385s - loss: 2.5866 - acc: 0.0285 - mean_absolute_error: 11.2949 - val_loss: 3.0742 - val_acc: 0.0189 - val_mean_absolute_error: 13.7363\n",
    "\n",
    "Epoch 4/20\n",
    "713/713 [==============================] - 386s - loss: 2.5862 - acc: 0.0292 - mean_absolute_error: 11.3336 - val_loss: 1.9153 - val_acc: 0.0334 - val_mean_absolute_error: 9.7895\n",
    "\n",
    "Epoch 5/20\n",
    "713/713 [==============================] - 386s - loss: 2.6258 - acc: 0.0278 - mean_absolute_error: 11.3908 - val_loss: 2.3101 - val_acc: 0.0185 - val_mean_absolute_error: 11.5918\n",
    "\n",
    "Epoch 6/20\n",
    "713/713 [==============================] - 386s - loss: 2.5469 - acc: 0.0292 - mean_absolute_error: 11.2433 - val_loss: 7.3051 - val_acc: 0.0384 - val_mean_absolute_error: 10.0754\n",
    "\n",
    "Epoch 7/20\n",
    "713/713 [==============================] - 385s - loss: 2.5583 - acc: 0.0292 - mean_absolute_error: 11.2443 - val_loss: 6.9639 - val_acc: 0.0416 - val_mean_absolute_error: 10.8122\n",
    "\n",
    "Epoch 8/20\n",
    "713/713 [==============================] - 386s - loss: 2.5845 - acc: 0.0273 - mean_absolute_error: 11.3074 - val_loss: 7.7042 - val_acc: 0.0330 - val_mean_absolute_error: 11.1933\n",
    "\n",
    "Epoch 9/20\n",
    "713/713 [==============================] - 385s - loss: 2.5135 - acc: 0.0310 - mean_absolute_error: 11.1403 - val_loss: 6.8166 - val_acc: 0.0384 - val_mean_absolute_error: 11.4688\n",
    "\n",
    "Epoch 10/20\n",
    "713/713 [==============================] - 386s - loss: 2.3537 - acc: 0.0281 - mean_absolute_error: 10.8007 - val_loss: 7.8546 - val_acc: 0.0358 - val_mean_absolute_error: 11.1792\n",
    "\n",
    "Epoch 11/20\n",
    "713/713 [==============================] - 385s - loss: 2.2141 - acc: 0.0310 - mean_absolute_error: 10.4251 - val_loss: 6.0965 - val_acc: 0.0382 - val_mean_absolute_error: 9.4952\n",
    "\n",
    "Epoch 12/20\n",
    "713/713 [==============================] - 385s - loss: 2.1059 - acc: 0.0326 - mean_absolute_error: 10.1197 - val_loss: 11.5091 - val_acc: 0.0446 - val_mean_absolute_error: 10.5134\n",
    "\n",
    "Epoch 13/20\n",
    "713/713 [==============================] - 386s - loss: 2.0857 - acc: 0.0334 - mean_absolute_error: 10.0760 - val_loss: 12.5309 - val_acc: 0.0436 - val_mean_absolute_error: 11.5219\n",
    "\n",
    "Epoch 14/20\n",
    "713/713 [==============================] - 386s - loss: 2.0577 - acc: 0.0334 - mean_absolute_error: 10.0021 - val_loss: 15.5565 - val_acc: 0.0400 - val_mean_absolute_error: 11.1698\n",
    "\n",
    "Epoch 15/20\n",
    "713/713 [==============================] - 386s - loss: 2.0236 - acc: 0.0343 - mean_absolute_error: 9.8573 - val_loss: 5.3131 - val_acc: 0.0388 - val_mean_absolute_error: 9.0751\n",
    "\n",
    "Epoch 16/20\n",
    "713/713 [==============================] - 386s - loss: 1.9644 - acc: 0.0344 - mean_absolute_error: 9.7395 - val_loss: 8.4828 - val_acc: 0.0334 - val_mean_absolute_error: 10.6123\n",
    "\n",
    "Epoch 17/20\n",
    "713/713 [==============================] - 386s - loss: 1.9953 - acc: 0.0345 - mean_absolute_error: 9.7768 - val_loss: 6.1084 - val_acc: 0.0378 - val_mean_absolute_error: 9.5485\n",
    "\n",
    "Epoch 18/20\n",
    "713/713 [==============================] - 385s - loss: 1.9610 - acc: 0.0351 - mean_absolute_error: 9.6949 - val_loss: 7.6045 - val_acc: 0.0382 - val_mean_absolute_error: 9.7756\n",
    "\n",
    "Epoch 19/20\n",
    "713/713 [==============================] - 386s - loss: 1.9285 - acc: 0.0355 - mean_absolute_error: 9.6024 - val_loss: 4.7563 - val_acc: 0.0372 - val_mean_absolute_error: 9.2489\n",
    "\n",
    "Epoch 20/20\n",
    "713/713 [==============================] - 385s - loss: 1.9519 - acc: 0.0358 - mean_absolute_error: 9.6799 - val_loss: 4.6270 - val_acc: 0.0444 - **val_mean_absolute_error: 9.0526**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
