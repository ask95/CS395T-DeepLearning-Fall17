{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Dropout, Flatten, Activation, Lambda, Input\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Lambda\n",
    "from keras import initializers\n",
    "from keras.utils import get_file\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "path = '/work/04381/ymarathe/maverick/yearbook/keras_yearbook/'\n",
    "data_path = '/work/04381/ymarathe/maverick/yearbook/'\n",
    "\n",
    "f = open(data_path + 'yearbook_train.txt', 'r')\n",
    "\n",
    "freq = {};\n",
    "fnamemap = {}\n",
    "year2idx = {}\n",
    "idx = 0\n",
    "normal_const = 0;\n",
    "\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    image, year = line.split(\"\\t\")\n",
    "    fnamemap[image] = year;\n",
    "    if year in freq:\n",
    "        freq[year] += 1\n",
    "    else:\n",
    "        freq[year] = 1\n",
    "\n",
    "normal_const = np.sum(freq.values())\n",
    "for key in freq:\n",
    "    freq[key] = freq[key]/float(normal_const);\n",
    "    \n",
    "sorted_freq = collections.OrderedDict(sorted(freq.items()))\n",
    "\n",
    "class_weights_train = {}\n",
    "idx2year = {}\n",
    "year2idx = {}\n",
    "\n",
    "for idx, key in enumerate(sorted_freq):\n",
    "    class_weights_train[idx] = sorted_freq[key]\n",
    "    idx2year[idx] = key\n",
    "    year2idx[key] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(data_path + 'yearbook_train.txt', 'r')\n",
    "\n",
    "fnameToGenderTr = {}\n",
    "\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    image, year = line.split(\"\\t\")\n",
    "    gender, imname = image.split(\"/\")\n",
    "    if gender is 'M':\n",
    "        encodeGender = 1\n",
    "    elif gender is 'F':\n",
    "        encodeGender = 0\n",
    "    fnameToGenderTr[image] = encodeGender\n",
    "\n",
    "f = open(data_path + 'yearbook_valid.txt', 'r')\n",
    "\n",
    "fnameToGenderVd = {}\n",
    "\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    image, year = line.split(\"\\t\")\n",
    "    gender, imname = image.split(\"/\")\n",
    "    if gender is 'M':\n",
    "        encodeGender = 1\n",
    "    elif gender is 'F':\n",
    "        encodeGender = 0\n",
    "    fnameToGenderVd[image] = encodeGender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batches(path, gen = ImageDataGenerator(), shuffle=True, class_mode=\"categorical\", batch_size=32, \n",
    "                target_size=(171, 186)):\n",
    "    return gen.flow_from_directory(path, shuffle=shuffle, batch_size=batch_size, target_size=target_size, \n",
    "                                   class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg19_conv_layers_sequential():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(171, 186, 3)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    vgg_pretrain_weights = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    \"https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n",
    "                                    cache_subdir='models')\n",
    "    \n",
    "    model.load_weights(vgg_pretrain_weights)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def vgg19_full_sequential():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(224, 224, 3)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    \n",
    "    vgg_pretrain_weights = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                                    'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                                    cache_subdir='models',\n",
    "                                                    file_hash='cbe5617147190e668d6c5d5026f83318')\n",
    "    \n",
    "    model.load_weights(vgg_pretrain_weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg19_conv_functional(img_input):\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.image import apply_transform, transform_matrix_offset_center\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import random_rotation, random_shear, random_shift, random_zoom\n",
    "from skimage import exposure\n",
    "data_path = '/work/04381/ymarathe/maverick/yearbook/'\n",
    "\n",
    "class ClassDataGen:\n",
    "    def __init__(self, directory, map_file, \n",
    "                 target_size = (171, 186, 3), \n",
    "                 class_weights_train = None, \n",
    "                 multi_output=False, \n",
    "                 do_augmentation=True, \n",
    "                 samplewise_center = True,\n",
    "                 samplewise_std_deviation = True,\n",
    "                 year2idx = None\n",
    "                ):\n",
    "        self.directory = directory\n",
    "        self.map_file = map_file\n",
    "        self.filenames = []\n",
    "        self.map = {}\n",
    "        self.fnameToGender = {}\n",
    "        self.target_size = target_size\n",
    "        self.populate_filenames()\n",
    "        self.populate_mapping()\n",
    "        self.regressIter = None\n",
    "        self.steps = 0\n",
    "        self.samplewise_center = samplewise_center\n",
    "        self.samplewise_std_deviation = samplewise_std_deviation\n",
    "        self.height_shift_range = 0.2\n",
    "        self.width_shift_range = 0.2\n",
    "        self.max_rotation = 45\n",
    "        self.shear = 0.785398\n",
    "        self.zoom_range = (0.5, 0.5)\n",
    "        self.do_augmentation = do_augmentation\n",
    "        self.class_weights_train = class_weights_train\n",
    "        self.equalizehist = False\n",
    "        self.multi_output = multi_output\n",
    "        self.lastN = []\n",
    "        self.year2idx = year2idx;\n",
    "        \n",
    "    def _recursive_list(self, subpath):\n",
    "        return sorted(\n",
    "            os.walk(subpath, followlinks=False), key=lambda tpl: tpl[0])\n",
    "    \n",
    "    def populate_mapping(self):\n",
    "        f = open(self.map_file, 'r')\n",
    "\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            image, year = line.split(\"\\t\")\n",
    "            gender, imfilename = image.split(\"/\")\n",
    "            if gender is 'M':\n",
    "                encodeGender = 1\n",
    "            elif gender is 'F':\n",
    "                encodeGender = 0\n",
    "            self.fnameToGender[image] = encodeGender\n",
    "            self.map[image] = year\n",
    "            \n",
    "    def populate_filenames(self):\n",
    "        base_dir = self.directory\n",
    "        for root, _, files in self._recursive_list(base_dir):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith('.' + 'png'):\n",
    "                    self.filenames.append(os.path.relpath(os.path.join(root, fname), base_dir))\n",
    "                    \n",
    "    def preprocess(self, x):\n",
    "        if self.equalizehist:\n",
    "            x = exposure.equalize_hist(x)\n",
    "            \n",
    "        return x\n",
    "            \n",
    "    def augment_data(self, x):\n",
    "        \n",
    "        x = random_shift(x, self.width_shift_range, self.height_shift_range, \n",
    "                         row_axis=0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_rotation(x, self.max_rotation, \n",
    "                            row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_shear(x, self.shear, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_zoom(x, self.zoom_range, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "    def flow_from_directory(self, batch_size = 32, shuffle = True, seed = 42):\n",
    "        \n",
    "        self.regressIter = Iterator(len(self.filenames), batch_size = batch_size, shuffle = shuffle, seed = seed)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            factor = 3\n",
    "        else:\n",
    "            factor = 1\n",
    "        \n",
    "        self.steps = math.ceil(len(self.filenames)/batch_size) * factor\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def oneHotEncodeYear(self, year):\n",
    "        integerEncoded = self.year2idx[year]\n",
    "        oneHotVec = [0 for _ in range(len(self.year2idx))]\n",
    "        oneHotVec[integerEncoded] = 1\n",
    "        return oneHotVec\n",
    "    \n",
    "    def oneHotEncodeGender(self, gender):\n",
    "        oneHotVec = [0, 0]\n",
    "        oneHotVec[gender] = 1;\n",
    "        return oneHotVec\n",
    "    \n",
    "    def next(self, *args, **kwargs):\n",
    "           \n",
    "        self.lastN = []\n",
    "        \n",
    "        idx_array, cur_idx, bs = next(self.regressIter.index_generator)\n",
    "        \n",
    "        batch_x = np.zeros(tuple([len(idx_array)] + list(self.target_size)), dtype=K.floatx())\n",
    "        \n",
    "        batch_y = np.zeros(tuple([len(idx_array)] + list([len(self.year2idx)])), dtype=K.floatx())\n",
    "        \n",
    "        if self.multi_output:\n",
    "            batch_y_gender = np.zeros(tuple([len(idx_array)] + list([2])), dtype=K.floatx())\n",
    "\n",
    "        if self.class_weights_train is not None:\n",
    "            sample_weights = np.ones(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        for i, j in enumerate(idx_array):\n",
    "            fname = self.filenames[j]\n",
    "            self.lastN.append(fname)\n",
    "            img = load_img(\n",
    "                  os.path.join(self.directory, fname),\n",
    "                  grayscale = True,\n",
    "                  target_size= self.target_size)\n",
    "            x = np.array(img_to_array(img, data_format='channels_last'))\n",
    "            x = self.preprocess(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i, :] = self.oneHotEncodeYear(self.map[fname])\n",
    "            \n",
    "            if self.multi_output:\n",
    "                batch_y_gender[i, :] = self.oneHotEncodeGender(self.fnameToGender[fname])\n",
    "            \n",
    "            if self.class_weights_train is not None:\n",
    "                if self.multi_output:\n",
    "                    sample_weights[i] = self.class_weights_train[self.map[fname]]\n",
    "                else:\n",
    "                    sample_weights[i] = self.class_weights_train[self.map[fname]]\n",
    "        \n",
    "        if self.samplewise_center:\n",
    "            for x in batch_x:\n",
    "                x -= np.mean(x)\n",
    "        \n",
    "        if self.samplewise_std_deviation:\n",
    "            for x in batch_x:\n",
    "                x /= np.std(x)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            for x in batch_x:\n",
    "                x = self.augment_data(x)\n",
    "        \n",
    "        if self.multi_output:\n",
    "            if self.class_weights_train is not None:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}, {'out_year' : sample_weights, 'out_gender' : sample_weights} \n",
    "            else:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}\n",
    "        else:    \n",
    "            if self.class_weights_train is not None:\n",
    "                return (batch_x, batch_y, sample_weights)\n",
    "            else:\n",
    "                return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = ClassDataGen(data_path + 'train',\n",
    "                       data_path + 'yearbook_train.txt', \n",
    "                       class_weights_train = sorted_freq, \n",
    "                       multi_output=True,\n",
    "                       do_augmentation=True,\n",
    "                       year2idx = year2idx\n",
    "                       )\n",
    "\n",
    "valid = ClassDataGen(data_path + 'valid',\n",
    "                       data_path + 'yearbook_valid.txt',\n",
    "                       class_weights_train = sorted_freq,\n",
    "                       do_augmentation=False,\n",
    "                       multi_output=True,\n",
    "                       year2idx = year2idx\n",
    "                      )\n",
    "\n",
    "train = train.flow_from_directory()\n",
    "valid = valid.flow_from_directory(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 171, 186, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)            (None, 171, 186, 64)  1792        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)            (None, 171, 186, 64)  36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 85, 93, 64)    0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)            (None, 85, 93, 128)   73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)            (None, 85, 93, 128)   147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 42, 46, 128)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)            (None, 42, 46, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)            (None, 42, 46, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)            (None, 42, 46, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)            (None, 42, 46, 256)   590080      block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 21, 23, 256)   0           block3_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)            (None, 21, 23, 512)   1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)            (None, 21, 23, 512)   2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)            (None, 21, 23, 512)   2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)            (None, 21, 23, 512)   2359808     block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 10, 11, 512)   0           block4_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)            (None, 10, 11, 512)   2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)            (None, 10, 11, 512)   2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)            (None, 10, 11, 512)   2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)            (None, 10, 11, 512)   2359808     block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 5, 5, 512)     0           block5_conv4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 12800)         0           block5_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          52432896    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 4096)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 4096)          16384       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "out_year (Dense)                 (None, 104)           426088      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "out_gender (Dense)               (None, 2)             8194        dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 72,907,946\n",
      "Trainable params: 72,899,754\n",
      "Non-trainable params: 8,192\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import merge\n",
    "\n",
    "img_input = Input(shape=(171, 186, 3))\n",
    "x = vgg19_conv_functional(img_input)\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, kernel_initializer='glorot_normal', bias_initializer=keras.initializers.Ones())(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out_year = Dense(104, activation='softmax', name = 'out_year')(x)\n",
    "out_gender = Dense(2, activation='softmax', name = 'out_gender')(x)\n",
    "\n",
    "def customMetric(y_true, y_pred):\n",
    "    return K.mean(K.abs(int(idx2year[np.nonzero(y_true)[0][0]]) - int(idx2year[np.nonzero(y_pred)[0][0]])))\n",
    "\n",
    "model = Model([img_input], [out_year, out_gender])\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "model.compile(Adam(lr=lr), loss=['categorical_crossentropy', 'categorical_crossentropy'], \n",
    "              metrics=['accuracy'],\n",
    "              loss_weights = [0.7, 0.3]\n",
    "            )\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return lr * (0.1 ** int(epoch / 10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    print (\"Saving result in exp45.h5\")\n",
    "    model.fit_generator(train, steps_per_epoch = train.steps, epochs = 5,                                \n",
    "                               validation_data = valid, \n",
    "                               validation_steps = valid.steps,\n",
    "                               callbacks=[LearningRateScheduler(lr_schedule),\n",
    "                               ModelCheckpoint(path + 'weights/exp45.h5', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(path + 'weights/exp44.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid.flow_from_directory(shuffle = False, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "mae = 0\n",
    "N = len(valid.filenames)\n",
    "for x in range(len(valid.filenames)):\n",
    "    batch_x, batch_y, sample_weights = next(valid)\n",
    "    true_year = int(idx2year[np.argmax(batch_y['out_year'])])\n",
    "    y_pred_prob = model.predict(batch_x)\n",
    "    pred_year = int(idx2year[np.argmax(y_pred_prob[0])])\n",
    "    y_true.append(true_year)\n",
    "    y_pred.append(pred_year)\n",
    "    mae += abs(true_year - pred_year)\n",
    "\n",
    "print mae/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path + 'weights/exp44_cm.npy', confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
