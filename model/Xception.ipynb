{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Dropout, Flatten, Activation, Lambda, Input\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Lambda\n",
    "from keras import initializers\n",
    "from keras.utils import get_file\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "data_path = '../data/yearbook/'\n",
    "\n",
    "f = open(data_path + 'yearbook_train.txt', 'r')\n",
    "\n",
    "freq = {};\n",
    "normal_const = 0;\n",
    "\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    image, year = line.split(\"\\t\")\n",
    "    if year in freq:\n",
    "        freq[year] += 1\n",
    "    else:\n",
    "        freq[year] = 1\n",
    "\n",
    "normal_const = np.sum(freq.values())\n",
    "for key in freq:\n",
    "    freq[key] = freq[key]/float(normal_const);\n",
    "    \n",
    "sorted_freq = collections.OrderedDict(sorted(freq.items()))\n",
    "\n",
    "idx = 0;\n",
    "class_weights_train = {}\n",
    "idx2year = {}\n",
    "\n",
    "for key in sorted_freq:\n",
    "    class_weights_train[idx] = sorted_freq[key]\n",
    "    idx2year[idx] = key\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.image import apply_transform, transform_matrix_offset_center\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import random_rotation, random_shear, random_shift, random_zoom\n",
    "from skimage import exposure\n",
    "\n",
    "class RegressDataGen:\n",
    "    def __init__(self, directory, map_file, target_size = (171, 186, 3), \n",
    "                 class_weights_train = None, multi_output=False, do_augmentation=True, \n",
    "                 samplewise_center = True,\n",
    "                 samplewise_std_deviation = True,\n",
    "                 multi_input=False\n",
    "                ):\n",
    "        self.directory = directory\n",
    "        self.map_file = map_file\n",
    "        self.filenames = []\n",
    "        self.map = {}\n",
    "        self.fnameToGender = {}\n",
    "        self.target_size = target_size\n",
    "        self.populate_filenames()\n",
    "        self.populate_mapping()\n",
    "        self.regressIter = None\n",
    "        self.steps = 0\n",
    "        self.samplewise_center = samplewise_center\n",
    "        self.samplewise_std_deviation = samplewise_std_deviation\n",
    "        self.height_shift_range = 0.2\n",
    "        self.width_shift_range = 0.2\n",
    "        self.max_rotation = 45\n",
    "        self.shear = 0.785398\n",
    "        self.zoom_range = (0.5, 0.5)\n",
    "        self.do_augmentation = do_augmentation\n",
    "        self.class_weights_train = class_weights_train\n",
    "        self.equalizehist = False\n",
    "        self.multi_output = multi_output\n",
    "        self.multi_input = multi_input\n",
    "        self.lastN = []\n",
    "        \n",
    "    def _recursive_list(self, subpath):\n",
    "        return sorted(\n",
    "            os.walk(subpath, followlinks=False), key=lambda tpl: tpl[0])\n",
    "    \n",
    "    def populate_mapping(self):\n",
    "        f = open(self.map_file, 'r')\n",
    "\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            image, year = line.split(\"\\t\")\n",
    "            gender, imfilename = image.split(\"/\")\n",
    "            if gender is 'M':\n",
    "                encodeGender = 1\n",
    "            elif gender is 'F':\n",
    "                encodeGender = 0\n",
    "            self.fnameToGender[image] = encodeGender\n",
    "            self.map[image] = year\n",
    "            \n",
    "    def populate_filenames(self):\n",
    "        base_dir = self.directory\n",
    "        for root, _, files in self._recursive_list(base_dir):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith('.' + 'png'):\n",
    "                    self.filenames.append(os.path.relpath(os.path.join(root, fname), base_dir))\n",
    "                    \n",
    "    def preprocess(self, x):\n",
    "        if self.equalizehist:\n",
    "            x = exposure.equalize_hist(x)\n",
    "\n",
    "        return x\n",
    "            \n",
    "    def augment_data(self, x):\n",
    "        \n",
    "        x = random_shift(x, self.width_shift_range, self.height_shift_range, \n",
    "                         row_axis=0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_rotation(x, self.max_rotation, \n",
    "                            row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_shear(x, self.shear, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        x = random_zoom(x, self.zoom_range, row_axis = 0, col_axis = 1, channel_axis = 2)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "    def flow_from_directory(self, batch_size = 32, shuffle = True, seed = 42):\n",
    "        \n",
    "        self.regressIter = Iterator(len(self.filenames), batch_size = batch_size, shuffle = shuffle, seed = seed)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            factor = 3\n",
    "        else:\n",
    "            factor = 1\n",
    "        \n",
    "        self.steps = math.ceil(len(self.filenames)/batch_size) * factor\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def next(self, *args, **kwargs):\n",
    "           \n",
    "        self.lastN = []\n",
    "        \n",
    "        idx_array, cur_idx, bs = next(self.regressIter.index_generator)\n",
    "        \n",
    "        batch_x = np.zeros(tuple([len(idx_array)] + list(self.target_size)), dtype=K.floatx())\n",
    "        \n",
    "        batch_y = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        if self.multi_output:\n",
    "            batch_y_gender = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "    \n",
    "        if self.multi_input:\n",
    "            batch_x_gender = np.zeros(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        if self.class_weights_train is not None:\n",
    "            sample_weights = np.ones(tuple([len(idx_array)]), dtype=K.floatx())\n",
    "        \n",
    "        for i, j in enumerate(idx_array):\n",
    "            fname = self.filenames[j]\n",
    "            self.lastN.append(fname)\n",
    "            img = load_img(\n",
    "                  os.path.join(self.directory, fname),\n",
    "                  grayscale = False,\n",
    "                  target_size= self.target_size)\n",
    "            x = np.array(img_to_array(img, data_format='channels_last'))\n",
    "            x = self.preprocess(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = self.map[fname]\n",
    "            \n",
    "            if self.multi_output:\n",
    "                batch_y_gender[i] = self.fnameToGender[fname]\n",
    "            \n",
    "            if self.multi_input:\n",
    "                batch_x_gender[i] = self.fnameToGender[fname]\n",
    "            \n",
    "            if self.class_weights_train is not None:\n",
    "                if self.multi_output:\n",
    "                    sample_weights[i] = self.class_weights_train[batch_y[i].astype('int').astype('str')]\n",
    "                else:\n",
    "                    sample_weights[i] = self.class_weights_train[batch_y[i].astype('int').astype('str')]\n",
    "        \n",
    "        if self.samplewise_center:\n",
    "            for x in batch_x:\n",
    "                x -= np.mean(x)\n",
    "        \n",
    "        if self.samplewise_std_deviation:\n",
    "            for x in batch_x:\n",
    "                x /= np.std(x)\n",
    "        \n",
    "        if self.do_augmentation:\n",
    "            for x in batch_x:\n",
    "                x = self.augment_data(x)\n",
    "        \n",
    "        if self.multi_output:\n",
    "            if self.class_weights_train is not None:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}, {'out_year' : sample_weights, 'out_gender' : sample_weights} \n",
    "            else:\n",
    "                return batch_x, {'out_year' : batch_y, 'out_gender': batch_y_gender}\n",
    "            \n",
    "        elif self.multi_input:\n",
    "            if self.class_weights_train is not None:\n",
    "                return {'input_1' : batch_x, 'input_2': batch_x_gender}, batch_y, sample_weights\n",
    "            else:\n",
    "                return {'input_1' : batch_x, 'input_2': batch_x_gender}, batch_y\n",
    "        else:    \n",
    "            if self.class_weights_train is not None:\n",
    "                return (batch_x, batch_y, sample_weights)\n",
    "            else:\n",
    "                return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "train = RegressDataGen(data_path + 'train',\n",
    "                       data_path + 'yearbook_train.txt', \n",
    "                       class_weights_train = sorted_freq,\n",
    "                       do_augmentation = False,\n",
    "                      )\n",
    "valid = RegressDataGen(data_path + 'valid',\n",
    "                       data_path + 'yearbook_valid.txt',\n",
    "                       class_weights_train = sorted_freq, \n",
    "                       do_augmentation = False,\n",
    "                      )\n",
    "\n",
    "train = train.flow_from_directory()\n",
    "valid = valid.flow_from_directory(shuffle=False)\n",
    "\n",
    "mean_value = 0\n",
    "for key in freq:\n",
    "    mean_value += freq[key] * float(key)\n",
    "    \n",
    "pretrained_model = Xception(include_top=False, weights='imagenet', input_shape=(171, 186, 3))\n",
    "x = pretrained_model.output\n",
    "x = Conv2D(16, (1, 1), activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu', bias_initializer=keras.initializers.Ones())(x)\n",
    "predicted_year = Dense(1, bias_initializer = keras.initializers.Constant(mean_value))(x)\n",
    "\n",
    "model = Model(inputs=pretrained_model.input, outputs=predicted_year)\n",
    "\n",
    "lr = 1e-3\n",
    "def lr_schedule(epoch):\n",
    "    return lr * (0.1 ** float(epoch / 10.0))\n",
    "\n",
    "model.compile(Adam(lr=lr), loss='mae', metrics=['mae'])\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "num_experiment = 12\n",
    "if training:\n",
    "    with tf.device('/gpu:0'):\n",
    "        filename = \"will_{}.h5\".format(num_experiment)\n",
    "        print (\"Starting experiment \" + str(num_experiment))\n",
    "        model.fit_generator(train, steps_per_epoch = train.steps, epochs = 20,                                \n",
    "                                   validation_data = valid, \n",
    "                                   validation_steps = valid.steps,\n",
    "                                   callbacks=[LearningRateScheduler(lr_schedule),\n",
    "                                ModelCheckpoint(data_path + filename, save_best_only=True)]\n",
    "                           )\n",
    "        print(\"Saved \" + filename)\n",
    "        \n",
    "else:\n",
    "    model.load_weights(data_path + 'will_{}.h5'.format(num_experiment))\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate_generator(valid, valid.steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception experiments\n",
    "\n",
    "### Warning: Experiment 6, 7, 8 had bad learning rates\n",
    "\n",
    "will_6.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 16\n",
    "\n",
    "Epoch 8/20\n",
    "713/713 [==============================] - 640s - loss: 6.6840 - mean_absolute_error: 20.0524 - val_loss: 3.9468 - val_mean_absolute_error: 16.9953\n",
    "\n",
    "will_7.h5:\n",
    "- loss = mae\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 16\n",
    "\n",
    "Epoch 3/20\n",
    "713/713 [==============================] - 642s - loss: 0.2597 - mean_absolute_error: 19.9808 - val_loss: 0.1756 - val_mean_absolute_error: 17.1463\n",
    "\n",
    "\n",
    "will_8.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-1 with lr decay\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 5/20\n",
    "713/713 [==============================] - 640s - loss: 6.6749 - mean_absolute_error: 20.0370 - val_loss: 3.9561 - val_mean_absolute_error: 16.9929\n",
    "\n",
    "\n",
    "will_9.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "713/713 [==============================] - 640s - loss: 0.0052 - mean_absolute_error: 0.5249 - val_loss: 0.2931 - val_mean_absolute_error: 3.6388\n",
    "\n",
    "will_10.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "713/713 [==============================] - 648s - loss: 0.0275 - mean_absolute_error: 1.1997 - val_loss: 0.2914 - val_mean_absolute_error: 3.5716\n",
    "\n",
    "will_11.h5:\n",
    "- loss = mse\n",
    "- epochs = 6 wita data augmentation * 3\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "2139/2139 [==============================] - 1847s - loss: 0.0788 - mean_absolute_error: 1.8316 - val_loss: 0.3522 - val_mean_absolute_error: 3.9830\n",
    "\n",
    "will_12.h5:\n",
    "- loss = mae\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 16 filers\n",
    "- dense = 32\n",
    "\n",
    "713/713 [==============================] - 635s - loss: 0.0057 - mean_absolute_error: 0.6751 - val_loss: 0.0363 - val_mean_absolute_error: 3.4857\n",
    "\n",
    "will_13.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 1e-3 with lr decay\n",
    "- conv 1x1 = 8 filers\n",
    "- dense = 16\n",
    "\n",
    "713/713 [==============================] - 640s - loss: 0.0076 - mean_absolute_error: 0.6360 - val_loss: 0.2842 - val_mean_absolute_error: 3.5779\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19 experiments\n",
    "\n",
    "will_1.h5:\n",
    "- loss = mse\n",
    "- epochs = 3\n",
    "- lr = 1e-2\n",
    "- conv 1x1 = 64 filers\n",
    "- dense = 128\n",
    "\n",
    "Epoch 1/3\n",
    "2139/2139 [==============================] - 1777s - loss: 1.7294 - acc: 0.0433 - mean_absolute_error: 8.9744 - val_loss: 1.2350 - val_acc: 0.0403 - val_mean_absolute_error: 8.3663\n",
    "\n",
    "Epoch 2/3\n",
    "2139/2139 [==============================] - 1706s - loss: 1.0568 - acc: 0.0573 - mean_absolute_error: 6.9857 - val_loss: 1.3283 - val_acc: 0.0404 - val_mean_absolute_error: 8.6860\n",
    "\n",
    "Epoch 3/3\n",
    "2139/2139 [==============================] - 1704s - loss: 0.8709 - acc: 0.0644 - mean_absolute_error: 6.3560 - val_loss: 0.9722 - val_acc: 0.0547 - **val_mean_absolute_error: 7.2192**\n",
    "\n",
    "will_2.h5:\n",
    "- loss = mse\n",
    "- epochs = 1\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "Epoch 1/1\n",
    "2139/2139 [==============================] - 1718s - loss: 1.9574 - acc: 0.0403 - mean_absolute_error: 9.6086 - val_loss: 1.6574 - val_acc: 0.0375 - **val_mean_absolute_error: 9.5908**\n",
    "\n",
    "will_3.h5:\n",
    "- loss = mse\n",
    "- epochs = 3 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 32 filers\n",
    "- dense = 64\n",
    "\n",
    "\n",
    "Epoch 2/3\n",
    "713/713 [==============================] - 388s - loss: 1.8113 - acc: 0.0436 - mean_absolute_error: 9.3031 - val_loss: 1.4613 - val_acc: 0.0378 - **val_mean_absolute_error: 9.0060**\n",
    "\n",
    "will_4.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 50 filers\n",
    "- dense = 100\n",
    "\n",
    "Epoch 19/20\n",
    "713/713 [==============================] - 387s - loss: 0.7039 - acc: 0.0581 - mean_absolute_error: 5.7438 - val_loss: 0.9351 - val_acc: 0.0663 - **val_mean_absolute_error: 6.9641**\n",
    "\n",
    "will_5.h5:\n",
    "- loss = mse\n",
    "- epochs = 20 without data augmentation\n",
    "- lr = 3e-2\n",
    "- conv 1x1 = 50 filers\n",
    "- dense = 100\n",
    "- Dropout = 0.5\n",
    "\n",
    "Epoch 20/20\n",
    "713/713 [==============================] - 385s - loss: 1.9519 - acc: 0.0358 - mean_absolute_error: 9.6799 - val_loss: 4.6270 - val_acc: 0.0444 - **val_mean_absolute_error: 9.0526**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
